{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.snowpark\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark.session import Session\n",
    "from snowflake.snowpark import version as v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "with open('creds1.json') as f:\n",
    "    data = json.load(f)\n",
    "    USERNAME = data['user']\n",
    "    PASSWORD = data['password']\n",
    "    SF_ACCOUNT = data['account']\n",
    "    SF_WH = data['warehouse']\n",
    "\n",
    "CONNECTION_PARAMETERS = {\n",
    "   \"account\": SF_ACCOUNT,\n",
    "   \"user\": USERNAME,\n",
    "   \"password\": PASSWORD,\n",
    "}\n",
    "\n",
    "session = Session.builder.configs(CONNECTION_PARAMETERS).create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.use_warehouse('COMPUTE_WH')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select either 100 or 10 for the TPC-DS Dataset size to use below. See (https://docs.snowflake.com/en/user-guide/sample-data-tpcds.html)[here] for more information If you choose 100, I recommend >= 3XL warehouse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNOWFLAKE_SAMPLE_DB = 'SNOWFLAKE_SAMPLE_DATA' \n",
    "TPCDS_SCHEMA = 'TPCDS_SF10TCL'\n",
    "    \n",
    "customer = session.table(f'{SNOWFLAKE_SAMPLE_DB}.{TPCDS_SCHEMA}.customer')\n",
    "address = session.table(f'{SNOWFLAKE_SAMPLE_DB}.{TPCDS_SCHEMA}.customer_address')\n",
    "demo = session.table(f'{SNOWFLAKE_SAMPLE_DB}.{TPCDS_SCHEMA}.customer_demographics')\n",
    "hdemo = session.table(f'{SNOWFLAKE_SAMPLE_DB}.{TPCDS_SCHEMA}.HOUSEHOLD_DEMOGRAPHICS')\n",
    "income = session.table(f'{SNOWFLAKE_SAMPLE_DB}.{TPCDS_SCHEMA}.INCOME_BAND')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "We will aggregate sales by customer across all channels(web, store, catalogue) and join that to customer demographic data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"C_CUSTOMER_ID\"   |\"C_SALUTATION\"  |\"C_PREFERRED_CUST_FLAG\"  |\"C_CURRENT_ADDR_SK\"  |\"C_CURRENT_CDEMO_SK\"  |\"C_CURRENT_HDEMO_SK\"  |\"CA_CITY\"        |\"CA_STATE\"  |\"CA_LOCATION_TYPE\"  |\"CA_ADDRESS_SK\"  |\"CD_GENDER\"  |\"CD_MARITAL_STATUS\"  |\"CD_EDUCATION_STATUS\"  |\"CD_CREDIT_RATING\"  |\"CD_DEMO_SK\"  |\"HD_DEMO_SK\"  |\"HD_INCOME_BAND_SK\"  |\"IB_UPPER_BOUND\"  |\"IB_INCOME_BAND_SK\"  |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|AAAAAAAALAAHMGBA  |Ms.             |N                        |30505073             |329364                |995                   |Lakeview         |GA          |condo               |30505073         |F            |S                    |Secondary              |Unknown             |329364        |995           |16                   |160000            |16                   |\n",
      "|AAAAAAAAMAAHMGBA  |Miss            |Y                        |9057863              |880419                |3141                  |Harmony          |MI          |single family       |9057863          |M            |U                    |College                |Good                |880419        |3141          |2                    |20000             |2                    |\n",
      "|AAAAAAAANAAHMGBA  |Sir             |Y                        |12887154             |1020235               |2271                  |Bennett          |IN          |single family       |12887154         |M            |D                    |Advanced Degree        |Good                |1020235       |2271          |12                   |120000            |12                   |\n",
      "|AAAAAAAAOAAHMGBA  |Sir             |N                        |23237532             |46345                 |2115                  |Star             |MO          |single family       |23237532         |M            |D                    |Primary                |Low Risk            |46345         |2115          |16                   |160000            |16                   |\n",
      "|AAAAAAAAPAAHMGBA  |Mr.             |N                        |25790880             |1598373               |2450                  |Denmark          |GA          |apartment           |25790880         |M            |S                    |Unknown                |Low Risk            |1598373       |2450          |11                   |110000            |11                   |\n",
      "|AAAAAAAAABAHMGBA  |Mrs.            |Y                        |9737661              |311578                |3243                  |Pleasant Valley  |SC          |apartment           |9737661          |F            |W                    |Primary                |High Risk           |311578        |3243          |4                    |40000             |4                    |\n",
      "|AAAAAAAABBAHMGBA  |Dr.             |N                        |1958483              |1331691               |5887                  |Brownsville      |KY          |condo               |1958483          |M            |M                    |Secondary              |Unknown             |1331691       |5887          |8                    |80000             |8                    |\n",
      "|AAAAAAAADBAHMGBA  |Miss            |N                        |19495354             |1876709               |1055                  |Wildwood         |IA          |condo               |19495354         |M            |U                    |Primary                |Good                |1876709       |1055          |16                   |160000            |16                   |\n",
      "|AAAAAAAAEBAHMGBA  |Miss            |Y                        |14962899             |694575                |4737                  |Antioch          |GA          |single family       |14962899         |M            |D                    |2 yr Degree            |Good                |694575        |4737          |18                   |180000            |18                   |\n",
      "|AAAAAAAAFBAHMGBA  |Dr.             |N                        |26573021             |410180                |5824                  |Ashland          |VA          |apartment           |26573021         |F            |U                    |4 yr Degree            |Good                |410180        |5824          |5                    |50000             |5                    |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#select C_CUSTOMER_ID, c_salutation, C_PREFERRED_CUST_FLAG, \n",
    "#ca_city, ca_state, ca_location_type,\n",
    "#cd_gender, cd_marital_status, cd_education_status, cd_credit_rating,\n",
    "#ib_upper_bound \n",
    "#from\n",
    "#snowflake_sample_data.tpcds_sf10tcl.customer cu inner join snowflake_sample_data.tpcds_sf10tcl.customer_address ca\n",
    "#on cu.c_current_addr_sk = ca.ca_address_sk\n",
    "#inner join snowflake_sample_data.TPCDS_SF10TCL.customer_demographics cd\n",
    "#on cu.c_current_cdemo_sk = cd.cd_demo_sk\n",
    "#inner join snowflake_sample_data.TPCDS_SF10TCL.HOUSEHOLD_DEMOGRAPHICS hd\n",
    "#on cu.c_current_hdemo_sk = hd.hd_demo_sk\n",
    "#inner join snowflake_sample_data.TPCDS_SF10TCL.INCOME_BAND ib\n",
    "#on hd.hd_income_band_sk = ib.ib_income_band_sk\n",
    "\n",
    "customer = customer.select('C_CUSTOMER_ID', 'c_salutation', 'C_PREFERRED_CUST_FLAG','c_current_addr_sk','c_current_cdemo_sk','c_current_hdemo_sk')\n",
    "hdemo = hdemo.select('hd_demo_sk','hd_income_band_sk')\n",
    "income = income.select('ib_upper_bound','ib_income_band_sk')\n",
    "\n",
    "customer = customer.join(address.select('ca_city', 'ca_state', 'ca_location_type','ca_address_sk'), customer['c_current_addr_sk'] == address['ca_address_sk'])\n",
    "customer = customer.join(demo.select('cd_gender', 'cd_marital_status', 'cd_education_status', 'cd_credit_rating','cd_demo_sk'), customer['c_current_cdemo_sk'] == demo['cd_demo_sk'] )\n",
    "customer = customer.join(hdemo, customer['c_current_hdemo_sk'] == hdemo['hd_demo_sk'])\n",
    "customer = customer.join(income.select('ib_upper_bound','IB_INCOME_BAND_SK'), customer['hd_income_band_sk'] == income['ib_income_band_sk'])\n",
    "\n",
    "customer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql('CREATE DATABASE IF NOT EXISTS tpcds_xgboost').collect()\n",
    "session.sql('CREATE SCHEMA IF NOT EXISTS tpcds_xgboost.demo').collect()\n",
    "\n",
    "session.use_database('tpcds_xgboost')\n",
    "session.use_schema('demo')\n",
    "customer.write.mode('overwrite').save_as_table('feature_store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The version of package snowflake-snowpark-python in the local environment is 1.1.0, which does not fit the criteria for the requirement snowflake-snowpark-python. Your UDF might not work when the package version is different between the server and your local environment\n",
      "The version of package scikit-learn in the local environment is 1.2.1, which does not fit the criteria for the requirement scikit-learn. Your UDF might not work when the package version is different between the server and your local environment\n",
      "The version of package joblib in the local environment is 1.2.0, which does not fit the criteria for the requirement joblib. Your UDF might not work when the package version is different between the server and your local environment\n",
      "The version of package cachetools in the local environment is 5.3.0, which does not fit the criteria for the requirement cachetools. Your UDF might not work when the package version is different between the server and your local environment\n",
      "The version of package xgboost in the local environment is 1.7.3, which does not fit the criteria for the requirement xgboost. Your UDF might not work when the package version is different between the server and your local environment\n"
     ]
    }
   ],
   "source": [
    "session.add_packages('snowflake-snowpark-python', 'scikit-learn', 'pandas', 'numpy', 'joblib', 'cachetools', 'xgboost', 'joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='Stage area ML_MODELS successfully created.')]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sql('CREATE OR REPLACE STAGE ml_models ').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.pipeline import Pipeline\n",
    "#from sklearn.impute import SimpleImputer\n",
    "#from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "#from sklearn.metrics import mean_squared_error\n",
    "#from sklearn.compose import ColumnTransformer\n",
    "#from sklearn.impute import SimpleImputer\n",
    "#from sklearn import preprocessing as prep\n",
    "#from xgboost import XGBRegressor\n",
    "#import pandas as pd\n",
    "#import numpy as np\n",
    "#import joblib\n",
    "#import os\n",
    "\n",
    "\n",
    "#snowdf = session.table(\"feature_store\")\n",
    "#snowdf = snowdf.drop(['CUSTOMER_SK', 'C_CURRENT_HDEMO_SK', 'C_CURRENT_ADDR_SK', 'C_CUSTOMER_ID', 'CA_ADDRESS_SK', 'CD_DEMO_SK'])\n",
    "#snowdf1= snowdf.select('C_CUSTOMER_ID', 'C_SALUTATION', 'C_PREFERRED_CUST_FLAG', 'CA_CITY', 'CA_STATE', 'CA_LOCATION_TYPE', 'CD_GENDER', 'CD_MARITAL_STATUS', 'CD_EDUCATION_STATUS', 'CD_CREDIT_RATING', 'IB_UPPER_BOUND')\n",
    "#snowdf_train, snowdf_test, snowdf_rest = snowdf1.random_split([0.08, 0.02, 0.9], seed=82) \n",
    "\n",
    "# save the train and test sets as time stamped tables in Snowflake \n",
    "#snowdf_train.write.mode(\"overwrite\").save_as_table(\"tpcds_xgboost.demo.tpc_TRAIN\")\n",
    "#snowdf_test.write.mode(\"overwrite\").save_as_table(\"tpcds_xgboost.demo.tpc_TEST\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df_train = snowdf_train.to_pandas()\n",
    "#df_test = snowdf_test.to_pandas()\n",
    "#df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#imputer=SimpleImputer(missing_values=np.nan,strategy='most_frequent')\n",
    "#imputer.fit(df_train)\n",
    "#df_train=imputer.transform(df_train)\n",
    "#df_train = pd.DataFrame(imputer.fit_transform(df_train))\n",
    "\n",
    "#df_train.columns = ['C_CUSTOMER_ID', 'C_SALUTATION', 'C_PREFERRED_CUST_FLAG', 'CA_CITY', 'CA_STATE', 'CA_LOCATION_TYPE', \n",
    "#'CD_GENDER', 'CD_MARITAL_STATUS', 'CD_EDUCATION_STATUS', 'CD_CREDIT_RATING', 'IB_UPPER_BOUND']\n",
    "\n",
    "\n",
    "#df_train = df_train.apply(prep.LabelEncoder().fit_transform)\n",
    "\n",
    "#df_train\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#imputer=SimpleImputer(missing_values=np.nan,strategy='most_frequent')\n",
    "#imputer.fit(df_test)\n",
    "#df_test=imputer.transform(df_test)\n",
    "#df_test = pd.DataFrame(imputer.fit_transform(df_test))\n",
    "#df_test.columns = ['C_CUSTOMER_ID', 'C_SALUTATION', 'C_PREFERRED_CUST_FLAG', 'CA_CITY', 'CA_STATE', 'CA_LOCATION_TYPE', \n",
    "#'CD_GENDER', 'CD_MARITAL_STATUS', 'CD_EDUCATION_STATUS', 'CD_CREDIT_RATING', 'IB_UPPER_BOUND']\n",
    "#df_test = df_test.apply(prep.LabelEncoder().fit_transform)\n",
    "\n",
    "#df_train\n",
    "#train_y = df_train['CD_CREDIT_RATING']\n",
    "#train_x = df_train.drop(columns=['CD_CREDIT_RATING'], axis = 1)\n",
    "#test_y = df_test['CD_CREDIT_RATING']\n",
    "#test_x = df_test.drop(columns = ['CD_CREDIT_RATING'], axis = 1)\n",
    "\n",
    "\n",
    "#train_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import preprocessing as prep\n",
    "from xgboost import XGBRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "def train_model(session: snowflake.snowpark.Session) -> float:\n",
    "    snowdf = session.table(\"feature_store\")\n",
    "    #snowdf = snowdf.drop(['CUSTOMER_SK', 'C_CURRENT_HDEMO_SK', 'C_CURRENT_ADDR_SK', 'C_CUSTOMER_ID', 'CA_ADDRESS_SK', 'CD_DEMO_SK'])\n",
    "    snowdf1= snowdf.select('C_CUSTOMER_ID', 'C_SALUTATION', 'C_PREFERRED_CUST_FLAG', 'CA_CITY', 'CA_STATE', 'CA_LOCATION_TYPE', \n",
    "    'CD_GENDER', 'CD_MARITAL_STATUS', 'CD_EDUCATION_STATUS', 'CD_CREDIT_RATING', 'IB_UPPER_BOUND')\n",
    "    snowdf_train, snowdf_test, snowdf_rest = snowdf1.random_split([0.08, 0.02, 0.9], seed=82) \n",
    "\n",
    "    # save the train and test sets as time stamped tables in Snowflake \n",
    "    snowdf_train.write.mode(\"overwrite\").save_as_table(\"tpcds_xgboost.demo.tpc_TRAIN\")\n",
    "    snowdf_test.write.mode(\"overwrite\").save_as_table(\"tpcds_xgboost.demo.tpc_TEST\")\n",
    "\n",
    "    df_train = snowdf_train.to_pandas()\n",
    "    df_test = snowdf_test.to_pandas()\n",
    "    \n",
    "    imputer=SimpleImputer(missing_values=np.nan,strategy='most_frequent')\n",
    "    imputer.fit(df_train)\n",
    "    df_train=imputer.transform(df_train)\n",
    "    df_train = pd.DataFrame(imputer.fit_transform(df_train))\n",
    "\n",
    "    df_train.columns = ['C_CUSTOMER_ID', 'C_SALUTATION', 'C_PREFERRED_CUST_FLAG', 'CA_CITY', 'CA_STATE', 'CA_LOCATION_TYPE', \n",
    "    'CD_GENDER', 'CD_MARITAL_STATUS', 'CD_EDUCATION_STATUS', 'CD_CREDIT_RATING', 'IB_UPPER_BOUND']\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    imputer=SimpleImputer(missing_values=np.nan,strategy='most_frequent')\n",
    "    imputer.fit(df_test)\n",
    "    df_test=imputer.transform(df_test)\n",
    "    df_test = pd.DataFrame(imputer.fit_transform(df_test))\n",
    "    df_test.columns = ['C_CUSTOMER_ID', 'C_SALUTATION', 'C_PREFERRED_CUST_FLAG', 'CA_CITY', 'CA_STATE', 'CA_LOCATION_TYPE', \n",
    "    'CD_GENDER', 'CD_MARITAL_STATUS', 'CD_EDUCATION_STATUS', 'CD_CREDIT_RATING', 'IB_UPPER_BOUND']\n",
    "\n",
    "\n",
    "    df_train = df_train.apply(prep.LabelEncoder().fit_transform)\n",
    "    df_test = df_test.apply(prep.LabelEncoder().fit_transform)\n",
    "\n",
    "    #df_train\n",
    "    train_y = df_train['IB_UPPER_BOUND']\n",
    "    train_x = df_train.drop(columns=['IB_UPPER_BOUND'], axis = 1)\n",
    "    test_y = df_test['IB_UPPER_BOUND']\n",
    "    test_x = df_test.drop(columns = ['IB_UPPER_BOUND'], axis = 1)\n",
    "\n",
    "    #train_x = snowdf_train.drop(\"cd_credit_rating\").to_pandas() # drop labels for training set\n",
    "    #train_y = snowdf_train.select(\"cd_credit_rating\").to_pandas()\n",
    "    #test_x = snowdf_test.drop(\"cd_credit_rating\").to_pandas()\n",
    "    #test_y = snowdf_test.select(\"cd_credit_rating\").to_pandas()\n",
    "    #cat_cols = ['C_CUSTOMER_ID', 'C_SALUTATION', 'C_PREFERRED_CUST_FLAG', 'CA_CITY', 'CA_STATE', 'CA_LOCATION_TYPE', 'CD_GENDER', 'CD_MARITAL_STATUS', 'CD_EDUCATION_STATUS']\n",
    "    #num_cols = ['IB_UPPER_BOUND']\n",
    "\n",
    "    #num_pipeline = Pipeline([\n",
    "    #        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "    #        ('std_scaler', StandardScaler()),\n",
    "    #    ])\n",
    "\n",
    "    #preprocessor = ColumnTransformer(\n",
    "    #transformers=[('num', num_pipeline, num_cols),\n",
    "    #              ('encoder', OneHotEncoder(handle_unknown=\"ignore\"), cat_cols) ])\n",
    "\n",
    "    cat_cols = ['C_CUSTOMER_ID', 'C_SALUTATION', 'C_PREFERRED_CUST_FLAG', 'CA_CITY', 'CA_STATE', 'CA_LOCATION_TYPE', 'CD_GENDER', 'CD_MARITAL_STATUS', 'CD_EDUCATION_STATUS','CD_CREDIT_RATING']\n",
    "    #num_cols = ['IB_UPPER_BOUND']\n",
    "    num_cols = []\n",
    "    \n",
    "    num_pipeline = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "            ('std_scaler', StandardScaler()),\n",
    "        ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "    transformers=[('num', num_pipeline, num_cols),\n",
    "                  ('encoder', OneHotEncoder(handle_unknown=\"ignore\"), cat_cols) ])\n",
    "\n",
    "    pipe = Pipeline([('preprocessor', preprocessor), \n",
    "                        ('xgboost', XGBRegressor())])\n",
    "    pipe.fit(train_x, train_y)\n",
    "#------------------------------------------------------\n",
    "    #pipe = Pipeline([('xgboost', XGBRegressor())])\n",
    "    pipe.fit(train_x, train_y)\n",
    "    pipe.fit(test_x, test_y)\n",
    "\n",
    "    test_preds = pipe.predict(test_x)\n",
    "    rmse = mean_squared_error(test_y, test_preds)\n",
    "    model_file = os.path.join('/tmp', 'model.joblib')\n",
    "    joblib.dump(pipe, model_file)\n",
    "    session.file.put(model_file, \"@ml_models\",overwrite=True)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33.13513074290939"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.use_warehouse('COMPUTE_WH')\n",
    "train_model_sp = F.sproc(train_model, session=session, replace=True, is_permanent=True, name=\"xgboost_sproc\", stage_location=\"@ml_models\")\n",
    "# Switch to Snowpark Optimized Warehouse for training and to run the stored proc\n",
    "train_model_sp(session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch back to feature engineering/inference warehouse\n",
    "session.use_warehouse('COMPUTE_WH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import cachetools\n",
    "import joblib\n",
    "from snowflake.snowpark import types as T\n",
    "\n",
    "session.add_import(\"@ml_models/model.joblib\")  \n",
    "\n",
    "features = ['CD_CREDIT_RATING', 'C_CUSTOMER_ID', 'C_SALUTATION', 'C_PREFERRED_CUST_FLAG', 'CA_CITY', 'CA_STATE', 'CA_LOCATION_TYPE', 'CD_GENDER', 'CD_MARITAL_STATUS', 'CD_EDUCATION_STATUS']\n",
    "\n",
    "@cachetools.cached(cache={})\n",
    "def read_file(filename):\n",
    "       import_dir = sys._xoptions.get(\"snowflake_import_directory\")\n",
    "       if import_dir:\n",
    "              with open(os.path.join(import_dir, filename), 'rb') as file:\n",
    "                     m = joblib.load(file)\n",
    "                     return m\n",
    "\n",
    "@F.pandas_udf(session=session, max_batch_size=10000, is_permanent=True, stage_location='@ml_models', replace=True, name=\"abcsk_xgboost_udf\") #name=\"clv_xgboost_udf\"\n",
    "def predict(df:  T.PandasDataFrame[str, str, str, str, str, str, str, str, str, str]) -> T.PandasSeries[float]:\n",
    "       m = read_file('model.joblib')       \n",
    "       df.columns = features\n",
    "       df = df.apply(prep.LabelEncoder().fit_transform)\n",
    "       return m.predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"C_CUSTOMER_ID\"   |\"C_SALUTATION\"  |\"C_PREFERRED_CUST_FLAG\"  |\"CA_CITY\"    |\"CA_STATE\"  |\"CA_LOCATION_TYPE\"  |\"CD_GENDER\"  |\"CD_MARITAL_STATUS\"  |\"CD_EDUCATION_STATUS\"  |\"CD_CREDIT_RATING\"  |\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|AAAAAAAAPGLNFACA  |Mr.             |N                        |Pine Grove   |ID          |apartment           |F            |W                    |Primary                |Unknown             |\n",
      "|AAAAAAAAAHLNFACA  |Miss            |Y                        |Oneida       |GA          |condo               |F            |S                    |Advanced Degree        |Unknown             |\n",
      "|AAAAAAAABHLNFACA  |Dr.             |Y                        |Spring Hill  |PA          |single family       |F            |S                    |College                |Good                |\n",
      "|AAAAAAAACHLNFACA  |Miss            |Y                        |Fairview     |WA          |apartment           |M            |D                    |2 yr Degree            |Low Risk            |\n",
      "|AAAAAAAADHLNFACA  |Sir             |Y                        |Wilson       |MN          |apartment           |F            |U                    |Primary                |High Risk           |\n",
      "|AAAAAAAAEHLNFACA  |Dr.             |Y                        |Spring Hill  |TX          |single family       |F            |S                    |Primary                |High Risk           |\n",
      "|AAAAAAAAFHLNFACA  |Dr.             |Y                        |Franklin     |RI          |apartment           |F            |D                    |2 yr Degree            |High Risk           |\n",
      "|AAAAAAAAGHLNFACA  |Miss            |Y                        |Greenwood    |NE          |apartment           |F            |W                    |Secondary              |Low Risk            |\n",
      "|AAAAAAAAHHLNFACA  |Miss            |Y                        |Valley View  |MO          |apartment           |M            |U                    |2 yr Degree            |Good                |\n",
      "|AAAAAAAAIHLNFACA  |Mrs.            |N                        |Antioch      |TX          |single family       |M            |W                    |4 yr Degree            |Unknown             |\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inference_df = session.table('feature_store')\n",
    "#inference_df = inference_df.select(\"C_CUSTOMER_ID\", \"c_salutation\", \"C_PREFERRED_CUST_FLAG\", \"ca_city\", \"ca_state\", \"ca_location_type\", \"cd_gender\", \"cd_marital_status\", \"cd_education_status\", \"cd_credit_rating\", \"ib_upper_bound\")\n",
    "#inference_df = inference_df.drop([ 'C_CURRENT_ADDR_SK', 'C_CURRENT_CDEMO_SK', 'C_CURRENT_HDEMO_SK', 'CA_ADDRESS_SK', 'CD_DEMO_SK','HD_DEMO_SK','HD_INCOME_BAND_SK','IB_INCOME_BAND_SK'])\n",
    "inference_df = inference_df.select('IB_UPPER_BOUND', 'C_CUSTOMER_ID', 'C_SALUTATION', 'C_PREFERRED_CUST_FLAG', 'CA_CITY', 'CA_STATE', 'CA_LOCATION_TYPE', 'CD_GENDER', 'CD_MARITAL_STATUS', 'CD_EDUCATION_STATUS','CD_CREDIT_RATING')\n",
    "inputs = inference_df.drop('IB_UPPER_BOUND')\n",
    "\n",
    "inputs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"C_CUSTOMER_ID\"   |\"C_SALUTATION\"  |\"C_PREFERRED_CUST_FLAG\"  |\"CA_CITY\"      |\"CA_STATE\"  |\"CA_LOCATION_TYPE\"  |\"CD_GENDER\"  |\"CD_MARITAL_STATUS\"  |\"CD_EDUCATION_STATUS\"  |\"CD_CREDIT_RATING\"  |\"PREDICTION\"        |\"IB_UPPER_BOUND\"  |\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|AAAAAAAALAAFHGBA  |Ms.             |Y                        |Georgetown     |KS          |single family       |M            |S                    |Unknown                |Unknown             |9.41344928741455    |150000            |\n",
      "|AAAAAAAAMAAFHGBA  |Miss            |N                        |Five Points    |MS          |condo               |M            |W                    |4 yr Degree            |High Risk           |9.49082088470459    |80000             |\n",
      "|AAAAAAAANAAFHGBA  |Ms.             |N                        |Kingston       |MN          |single family       |M            |D                    |2 yr Degree            |Good                |9.511343955993652   |10000             |\n",
      "|AAAAAAAAOAAFHGBA  |Dr.             |N                        |Kingston       |WI          |condo               |F            |D                    |College                |Unknown             |10.631597518920898  |50000             |\n",
      "|AAAAAAAAPAAFHGBA  |Dr.             |Y                        |Hillcrest      |TX          |condo               |M            |D                    |4 yr Degree            |High Risk           |9.49082088470459    |30000             |\n",
      "|AAAAAAAAABAFHGBA  |Mrs.            |Y                        |Oakland        |CO          |apartment           |M            |U                    |4 yr Degree            |Low Risk            |9.484292984008789   |60000             |\n",
      "|AAAAAAAABBAFHGBA  |Dr.             |N                        |Harmony        |TN          |apartment           |F            |D                    |4 yr Degree            |Unknown             |9.494288444519043   |110000            |\n",
      "|AAAAAAAACBAFHGBA  |Sir             |N                        |Pleasant Hill  |MA          |apartment           |M            |S                    |Advanced Degree        |Low Risk            |9.491445541381836   |20000             |\n",
      "|AAAAAAAADBAFHGBA  |Dr.             |N                        |Mountain View  |VA          |condo               |F            |U                    |Unknown                |Low Risk            |9.465682029724121   |170000            |\n",
      "|AAAAAAAAFBAFHGBA  |Miss            |Y                        |Riverside      |ND          |single family       |M            |W                    |College                |Good                |9.497973442077637   |160000            |\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "snowdf_results = inference_df.select(*inputs,\n",
    "                    predict(*inputs).alias('PREDICTION'), \n",
    "                    (F.col('IB_UPPER_BOUND')).alias('IB_UPPER_BOUND')\n",
    "                    )\n",
    "#snowdf_results.write.mode('overwrite').save_as_table('predictions1')\n",
    "snowdf_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61590306"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "could not allocate 469762048 bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[144], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39m# Train the Random Forest classifier\u001b[39;00m\n\u001b[0;32m     10\u001b[0m clf \u001b[39m=\u001b[39m RandomForestClassifier(n_estimators\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m clf\u001b[39m.\u001b[39;49mfit(train_x, train_y)\n\u001b[0;32m     13\u001b[0m \u001b[39m# Make predictions on the testing set\u001b[39;00m\n\u001b[0;32m     14\u001b[0m y_pred \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mpredict(test_x)\n",
      "File \u001b[1;32mc:\\Users\\susha\\anaconda3\\envs\\py38_env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:473\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    462\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[0;32m    463\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m    464\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    465\u001b[0m ]\n\u001b[0;32m    467\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    470\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 473\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[0;32m    474\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[0;32m    475\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    476\u001b[0m     prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    477\u001b[0m )(\n\u001b[0;32m    478\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    479\u001b[0m         t,\n\u001b[0;32m    480\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbootstrap,\n\u001b[0;32m    481\u001b[0m         X,\n\u001b[0;32m    482\u001b[0m         y,\n\u001b[0;32m    483\u001b[0m         sample_weight,\n\u001b[0;32m    484\u001b[0m         i,\n\u001b[0;32m    485\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[0;32m    486\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    487\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[0;32m    488\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[0;32m    489\u001b[0m     )\n\u001b[0;32m    490\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[0;32m    491\u001b[0m )\n\u001b[0;32m    493\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\susha\\anaconda3\\envs\\py38_env\\lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\susha\\anaconda3\\envs\\py38_env\\lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\susha\\anaconda3\\envs\\py38_env\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\susha\\anaconda3\\envs\\py38_env\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\susha\\anaconda3\\envs\\py38_env\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\susha\\anaconda3\\envs\\py38_env\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\susha\\anaconda3\\envs\\py38_env\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\susha\\anaconda3\\envs\\py38_env\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\susha\\anaconda3\\envs\\py38_env\\lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\susha\\anaconda3\\envs\\py38_env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:184\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[39melif\u001b[39;00m class_weight \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbalanced_subsample\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    182\u001b[0m         curr_sample_weight \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m compute_sample_weight(\u001b[39m\"\u001b[39m\u001b[39mbalanced\u001b[39m\u001b[39m\"\u001b[39m, y, indices\u001b[39m=\u001b[39mindices)\n\u001b[1;32m--> 184\u001b[0m     tree\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49mcurr_sample_weight, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    185\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     tree\u001b[39m.\u001b[39mfit(X, y, sample_weight\u001b[39m=\u001b[39msample_weight, check_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\susha\\anaconda3\\envs\\py38_env\\lib\\site-packages\\sklearn\\tree\\_classes.py:889\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    859\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    860\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    861\u001b[0m \n\u001b[0;32m    862\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    886\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 889\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    890\u001b[0m         X,\n\u001b[0;32m    891\u001b[0m         y,\n\u001b[0;32m    892\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    893\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    895\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\susha\\anaconda3\\envs\\py38_env\\lib\\site-packages\\sklearn\\tree\\_classes.py:379\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    370\u001b[0m         splitter,\n\u001b[0;32m    371\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    377\u001b[0m     )\n\u001b[1;32m--> 379\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight)\n\u001b[0;32m    381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[0;32m    382\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32msklearn\\tree\\_tree.pyx:147\u001b[0m, in \u001b[0;36msklearn.tree._tree.DepthFirstTreeBuilder.build\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msklearn\\tree\\_tree.pyx:242\u001b[0m, in \u001b[0;36msklearn.tree._tree.DepthFirstTreeBuilder.build\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msklearn\\tree\\_tree.pyx:747\u001b[0m, in \u001b[0;36msklearn.tree._tree.Tree._add_node\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msklearn\\tree\\_tree.pyx:718\u001b[0m, in \u001b[0;36msklearn.tree._tree.Tree._resize_c\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msklearn\\tree\\_utils.pyx:35\u001b[0m, in \u001b[0;36msklearn.tree._utils.safe_realloc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: could not allocate 469762048 bytes"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif, RFE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Train the Random Forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "clf.fit(train_x, train_y)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = clf.predict(test_x)\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "acc = accuracy_score(test_y, y_pred)\n",
    "prec = precision_score(test_y, y_pred, average='weighted')\n",
    "rec = recall_score(test_y, y_pred, average='weighted')\n",
    "f1 = f1_score(test_y, y_pred, average='weighted')\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"Precision:\", prec)\n",
    "print(\"Recall:\", rec)\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "# Tune the hyperparameters using GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='f1_weighted')\n",
    "grid_search.fit(train_x, train_y)\n",
    "print(\"Best parameters:\", grid_search.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1b19cb82ab226acb89327c07b606fc5a13de1995fea6d7a9ac06670799019229"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
